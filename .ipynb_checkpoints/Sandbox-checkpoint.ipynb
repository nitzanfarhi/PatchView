{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9aa2311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nitzan\\Anaconda3\\envs\\torch310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import code_training\n",
    "import events_models\n",
    "importlib.reload(events_models)\n",
    "importlib.reload(code_training)\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "\n",
    "sys.argv = [\"run.py\"]\n",
    "args = code_training.parse_args()\n",
    "args.cache_dir = \"cache_data\"\n",
    "args.model_type = \"roberta\"\n",
    "args.n_gpu = 1\n",
    "args.device = \"cpu\"\n",
    "if args.cache_dir:\n",
    "    args.model_cache_dir = os.path.join(args.cache_dir, \"models\")\n",
    "\n",
    "\n",
    "# tokenizer, model = code_training.get_text_model_and_tokenizer(args)\n",
    "\n",
    "with open(os.path.join(args.cache_dir,\"orc\", \"orchestrator.json\"), \"r\") as f:\n",
    "    mall = json.load(f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f4907a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\nitzan\\local\\analyzeCVE\\data_collection\\data\\repo_commits.json\", \"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a56efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import code_training\n",
    "import events_datasets\n",
    "\n",
    "importlib.reload(events_datasets)\n",
    "importlib.reload(code_training)\n",
    "\n",
    "code_args = argparse.Namespace(**vars(args))\n",
    "message_args = argparse.Namespace(**vars(args))\n",
    "events_args = argparse.Namespace(**vars(args))\n",
    "\n",
    "keys = sorted(list(mall.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd9ceede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "code_args.recreate_cache = True\n",
    "code_args.code_merge_file = True\n",
    "code_tokenizer = code_training.get_tokenizer(code_args)\n",
    "code_dataset = code_training.TextDataset(code_tokenizer, code_args, mall, keys, \"train\")\n",
    "code_model = code_training.get_text_model(code_args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf80b332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/codebert-base were not used when initializing RobertaForSequenceClassification: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at microsoft/codebert-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "message_args.recreate_cache = True\n",
    "message_args.code_merge_file = True\n",
    "message_tokenizer = code_training.get_tokenizer(message_args)\n",
    "message_dataset = code_training.TextDataset(message_tokenizer, message_args, mall, keys, \"train\")\n",
    "message_model = code_training.get_text_model(message_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f80f3c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \r"
     ]
    }
   ],
   "source": [
    "events_args.model_type = \"conv1d\"\n",
    "events_args.recreate_cache = True\n",
    "events_dataset = events_datasets.EventsDataset(events_args, mall, keys, \"train\")\n",
    "events_model = code_training.get_events_model(events_args, events_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3472ec83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 1078, in _pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\n",
      "  File \"_pydevd_bundle/pydevd_cython.pyx\", line 297, in _pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\n",
      "  File \"c:\\Users\\nitzan\\Anaconda3\\envs\\torch310\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py\", line 1976, in do_wait_suspend\n",
      "    keep_suspended = self._do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n",
      "  File \"c:\\Users\\nitzan\\Anaconda3\\envs\\torch310\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py\", line 2011, in _do_wait_suspend\n",
      "    time.sleep(0.01)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m KFold, train_test_split\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m KFold, train_test_split\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1363\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:662\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1087\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:1078\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_pydevd_bundle/pydevd_cython.pyx:297\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\nitzan\\Anaconda3\\envs\\torch310\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:1976\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[0;32m   1973\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[0;32m   1975\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001b[1;32m-> 1976\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[0;32m   1978\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1980\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[0;32m   1981\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nitzan\\Anaconda3\\envs\\torch310\\lib\\site-packages\\debugpy\\_vendored\\pydevd\\pydevd.py:2011\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[0;32m   2008\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_mpl_hook()\n\u001b[0;32m   2010\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[1;32m-> 2011\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[0;32m   2013\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[0;32m   2015\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler, random_split, SubsetRandomSampler\n",
    "import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "class MyConcatDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, code_dataset, message_dataset, events_dataset):\n",
    "        self.code_dataset = code_dataset\n",
    "        self.message_dataset = message_dataset\n",
    "        self.events_dataset = events_dataset\n",
    "\n",
    "        self.merged_dataset = []\n",
    "        self.merged_labels = []\n",
    "        self.merged_info = []\n",
    "        code_counter = 0\n",
    "        message_counter = 0\n",
    "        events_counter = 0\n",
    "\n",
    "        \n",
    "        while code_counter < len(code_dataset) and message_counter < len(message_dataset) and events_counter < len(events_dataset):\n",
    "            code_commit = code_dataset.final_commit_info[code_counter]\n",
    "            message_commit = message_dataset.final_commit_info[message_counter]\n",
    "            events_commit = events_dataset.final_commit_info[events_counter]\n",
    "\n",
    "            if code_commit[\"hash\"] == message_commit[\"hash\"] == events_commit[\"hash\"]:\n",
    "                self.merged_dataset.append((code_dataset[code_counter][0], message_dataset[code_counter][0], events_dataset[code_counter][0]))\n",
    "                self.merged_labels.append(code_dataset[code_counter][1])\n",
    "                self.merged_info.append(code_dataset.final_commit_info[code_counter])\n",
    "                code_counter += 1\n",
    "                message_counter += 1\n",
    "                events_counter += 1\n",
    "            elif code_commit[\"hash\"] < message_commit[\"hash\"]:\n",
    "                code_counter += 1\n",
    "            elif message_commit[\"hash\"] < events_commit[\"hash\"]:\n",
    "                message_counter += 1\n",
    "            else:\n",
    "                events_counter += 1\n",
    "\n",
    "\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        res = []\n",
    "        for d in self.datasets:\n",
    "            res.append(d[i][0])\n",
    "        label = self.datasets[0][i][1]\n",
    "        return res, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(d) for d in self.datasets)\n",
    "    \n",
    "\n",
    "concat_dataset = MyConcatDataset(code_dataset, message_dataset, events_dataset)\n",
    "args.device =  device = torch.device(\"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6187a57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000030feb7a30f193197f1aab8a7b04a26b42735 000030feb7a30f193197f1aab8a7b04a26b42735 000030feb7a30f193197f1aab8a7b04a26b42735\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "commit_a = code_dataset.final_commit_info[i]\n",
    "commit_b = message_dataset.final_commit_info[i]\n",
    "commit_c = events_dataset.final_commit_info[i]\n",
    "print(commit_a[\"hash\"] , commit_b[\"hash\"] , commit_c[\"hash\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cbfe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class MultiModel(nn.Module):\n",
    "    def __init__(self, code_model, events_model, message_model, args):\n",
    "        super(MultiModel, self).__init__()\n",
    "        self.code_model = code_model\n",
    "        self.message_model = message_model\n",
    "        self.events_model = events_model\n",
    "        self.args = args\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        self.classifier = nn.Linear(3, 2)\n",
    "\n",
    "    def forward(self, data, labels=None):\n",
    "        code, message, events = data\n",
    "        code = self.code_model(code)[0]\n",
    "        message = self.message_model(message)[0]\n",
    "        events = self.events_model(events)[0]\n",
    "        x = torch.stack([code, message, events], dim=1)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.classifier(x)\n",
    "        prob = torch.sigmoid(logits)\n",
    "        if labels is not None:\n",
    "            labels = labels.float()\n",
    "            loss = torch.log(prob[:, 0]+1e-10)*labels + \\\n",
    "                torch.log((1-prob)[:, 0]+1e-10)*(1-labels)\n",
    "            loss = -loss.mean()\n",
    "            return loss, prob\n",
    "        else:\n",
    "            return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cadedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiModel(code_model, message_model, events_model, args)\n",
    "\n",
    "tr_loss, logging_loss, avg_loss, tr_nb, tr_num, train_loss = 0.0, 0.0, 0.0, 0, 0, 0\n",
    "model.zero_grad()\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': args.weight_decay},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(\n",
    "        nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(\n",
    "    optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.max_steps*0.1,\n",
    "                                            num_training_steps=args.max_steps)\n",
    "\n",
    "args.start_epoch = 0\n",
    "args.start_step = 0\n",
    "args.source_model = \"Multi\"\n",
    "global_step = args.start_step\n",
    "\n",
    "train_dataloader = DataLoader(concat_dataset, batch_size=2, num_workers=0, pin_memory=True)\n",
    "for idx in range(args.start_epoch, int(args.num_train_epochs)):\n",
    "    bar = tqdm.tqdm(train_dataloader, total=len(train_dataloader))\n",
    "    tr_num = 0\n",
    "    train_loss = 0\n",
    "    for step, batch in enumerate(bar):\n",
    "        if args.source_model == \"Multi\":\n",
    "            inputs = [x.to(args.device) for x in batch[0]]\n",
    "        else:\n",
    "            inputs = batch[0].to(args.device)\n",
    "\n",
    "        labels = batch[1].to(args.device)\n",
    "        model.train()\n",
    "        loss, logits = model(inputs, labels)\n",
    "\n",
    "        if args.n_gpu > 1:\n",
    "            loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), args.max_grad_norm)\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        tr_num += 1\n",
    "        train_loss += loss.item()\n",
    "        if avg_loss == 0:\n",
    "            avg_loss = tr_loss\n",
    "        avg_loss = round(train_loss/tr_num, 5)\n",
    "        final_train_loss = avg_loss\n",
    "        bar.set_description(\"epoch {} loss {}\".format(idx, avg_loss))\n",
    "\n",
    "        if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            scheduler.step()\n",
    "            global_step += 1\n",
    "            output_flag = True\n",
    "            avg_loss = round(\n",
    "                np.exp((tr_loss - logging_loss) / (global_step - tr_nb)), 4)\n",
    "            if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                logging_loss = tr_loss\n",
    "                tr_nb = global_step\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
